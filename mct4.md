Morphogenic Compute Topology v4.0 (MCT4)
A Language-Agnostic Specification for a Self-Structuring, Continuously-Learning Compute Graph
1. The Central Argument
Gradient-based deep learning dominates not because backpropagation is the best learning algorithm, but because it was the first to combine sufficient expressiveness (deep nonlinear composition), efficient hardware mapping (batched matrix multiplication on GPUs), and a reliable enough credit assignment signal (chain rule) to make large-scale training tractable. Each of these three pillars has exploitable weaknesses:
Expressiveness: Fixed architectures are chosen before seeing data. The structure is a hyperparameter, not a learned variable.
Hardware mapping: Dense matrix operations waste compute on near-zero activations. Sparse biological neural systems perform orders of magnitude more operations per watt.
Credit assignment: Backpropagation requires locking all layer activations in memory until the backward pass completes. This is biologically implausible, prevents true online learning, and creates a hard memory scaling wall.
MCT4 attacks all three. It learns its own structure, executes sparsely, and assigns credit locally without storing a computation graph. The tradeoff is not capability ‚Äî it is familiarity. MCT4 requires different initialization intuitions and different infrastructure than PyTorch. The spec below is complete and self-contained.
2. Global State
2.1 Vector Dimensionality (D)
All node inputs, outputs, weight matrices, and the context vector operate at dimensionality D (e.g., 512 for medium tasks, 2048 for large). This is the single most important architectural constant.
2.2 Context Vector (C)
A real-valued vector C ‚àà ‚Ñù·¥∞, initialized to zero at the start of each sequence (not each token). It persists across forward passes within a sequence, functioning as a learned routing memory.
Each node that fails to fire contributes its near-miss potential as a ghost signal:

```
C ‚Üê C ¬∑ decay_c + (œÅ·µ¢ / D) ¬∑ S·µ¢
```

decay_c ‚àà (0,1) (e.g., 0.95). Ghost signals decay naturally. C is not reset between tokens in a sequence ‚Äî it carries forward temporal context. It resets only at sequence boundaries. This is the mechanism by which MCT4 handles sequential data without a separate recurrence architecture.
2.3 Latency Clock (t)
A monotonic hop counter reset at the start of every forward pass. Budget: t_budget (e.g., 20 hops).
Dynamic threshold:

```
œÑ(t) = exp(Œª_œÑ ¬∑ (t ‚àí t_budget))
```

As t ‚Üí t_budget, œÑ ‚Üí ‚àû, collapsing all routing to the shortest available output path. MCT4 is an anytime algorithm: interrupted at any point, it emits the best current result.
2.4 Convergence Monitor (Œ∫)
A global counter incremented each pass with zero pruning events, reset on any pruning. When Œ∫ > Œ∫_thresh, mutation noise and atrophy are halved. The graph shifts from structural exploration to parameter refinement. This is the equivalent of learning rate scheduling in gradient descent, but driven by structural stability rather than epoch count.
2.5 Batch State
MCT4 supports parallel execution of N samples through the same graph. Each node maintains N inbox slots and accumulates N output vectors. Health and weight updates aggregate over the batch after all N forward passes complete. This provides the same variance reduction benefits as mini-batch gradient descent. Batch size N=1 is valid and corresponds to true online learning ‚Äî a regime inaccessible to most gradient-based systems without significant engineering overhead.
3. The Node
Each node i is an asynchronous, self-contained processing unit with full parametric capacity.
Field Type Description id int Unique identifier S ‚Ñù·¥∞ Routing signature: geometric embedding used for activation potential œÅ_base ‚Ñù Health scalar: survival fitness and routing priority W ‚Ñù·¥∞À£·¥∞ Learnable weight matrix, initialized to identity √ó Œµ P enum Primitive nonlinearity (Section 7) inbox map sender_id ‚Üí [(vector, time_arrived)] ‚Äî N slots for batched execution edges_out list Target node IDs steps_idle int Passes since last firing tension_trace ‚Ñù Exponential moving average of ‚ÄñT‚Äñ observed at this node
The weight matrix W ‚àà ‚Ñù·¥∞À£·¥∞ gives each node the representational power of a full linear layer. This is the critical upgrade from a bias vector. A node with W initialized to identity and a nonlinear primitive P is equivalent to one layer of a residual network. The full MCT4 graph is therefore at least as expressive as a deep residual network of comparable depth, while being able to grow beyond it.
Memory note: For large D, W can be factored as W = A B·µÄ where A, B ‚àà ‚Ñù·¥∞À£ ≥ for rank r << D (e.g., r = 64). This reduces per-node memory from D¬≤ to 2Dr and learning updates to rank-1 outer products ‚Äî equivalent to LoRA-style factored adaptation. Full-rank and low-rank modes are both valid; choose based on resource constraints.
4. Phase 1 ‚Äî Forward Execution
For input X ‚àà ‚Ñù·¥∞, execute asynchronously via a priority queue ordered by hop count. Halt when any output node fires or t > t_budget.
Step 1: Activation Potential

```
œÅ·µ¢ = œÅ_base,i + dot(S·µ¢, X) + dot(S·µ¢, C)
```

The input X drives content-based routing. The context C biases routing based on accumulated near-miss history ‚Äî nodes that were close to firing on recent tokens are easier to activate now. Together they implement a form of dynamic attention over the graph without an explicit attention mechanism.
Step 2: Ghost Contribution (Fail to Fire)
If œÅ·µ¢ < œÑ(t):

```
C ‚Üê C ¬∑ decay_c + (œÅ·µ¢ / D) ¬∑ S·µ¢
```

Step 3: Inbox Processing (Fire)
If œÅ·µ¢ ‚â• œÑ(t), process inbox:

1. Decay each waiting vector: V_in ‚Üê V_in ¬∑ exp(‚àíŒª_async ¬∑ (t ‚àí time_arrived))

2. Arity check: if >50% of required input ports are filled, proceed. Zero-fill missing ports.

3. Aggregate N batch slots independently.

Step 4: Execution
Apply weight matrix, then primitive, then route:

```
V_out = P(W ¬∑ V_in)
```

The weight matrix transforms before the nonlinearity. This ordering (linear ‚Üí nonlinear) matches standard neural layer convention and ensures W operates on the full input signal before compression. Transmit V_out to all edges_out. Reset steps_idle to 0. Clear inbox.
5. Phase 2 ‚Äî Learning
MCT4 learning is local, online, and does not require storing a computation graph. There is no backward pass in the backpropagation sense ‚Äî there is a retrograde error signal that does the same work without the memory overhead.
Step 1: Tension

```
T_v = (Y* ‚àí Y) / ‚àöD         ‚Üê normalized error direction
‚ÄñT‚Äñ = MSE(Y*, Y) ‚àà [0,1]   ‚Üê scalar error magnitude
```

For batch execution, average T_v and ‚ÄñT‚Äñ over the N samples.
Step 2: Retrograde Flow
T_v propagates in reverse topological order up the active path. At each multi-input node, blame is partitioned by incoming signal magnitude:

```
w_blame,j = ‚ÄñV_in,j‚Äñ‚ÇÇ / (Œ£‚ÄñV_in‚Äñ‚ÇÇ + Œµ)
```

The attenuated tension signal reaching node i is:

```
T_local,i = T_v ¬∑ w_blame,i ¬∑ (1 ‚àí tension_trace_i ¬∑ 0.5)
```

The tension_trace term implements a local learning rate adaptation: nodes that have been consistently wrong attenuate their update step, equivalent to RMSProp-style variance normalization without global state.
Step 3: Weight Matrix Update
For each active node i, update W via the outer product of the error signal and the pre-activation input:

```
ŒîW = Œ∑ ¬∑ T_local,i ‚äó V_in,i      ‚Üê rank-1 update
W_i ‚Üê W_i + ŒîW
```

This is not a heuristic. It is the exact gradient of the local squared error ‚ÄñT_local,i ‚àí W_i V_in,i‚Äñ¬≤ with respect to W_i, equivalent to one step of stochastic gradient descent on the local loss. The retrograde flow approximates the chain rule by passing the error signal upstream; each node performs its exact local gradient step given what it receives. The approximation relative to full backpropagation is in the fidelity of the upstream signal, not in the local update rule itself.
For low-rank factored mode: A ‚Üê A + Œ∑ ¬∑ T_local,i ‚äó (B·µÄ V_in,i) and B ‚Üê B + Œ∑ ¬∑ V_in,i ‚äó (A·µÄ T_local,i).
After update, clamp ‚ÄñW‚Äñ_F ‚â§ W_max (e.g., W_max = ‚àöD) via spectral rescaling if exceeded. This prevents weight explosion without element-wise clamping, which would distort learned directions.
Step 4: Tension Trace Update

```
tension_trace_i ‚Üê 0.9 ¬∑ tension_trace_i + 0.1 ¬∑ ‚ÄñT_local,i‚Äñ
```

Step 5: Health Update

```
ŒîœÅ = Œ± ¬∑ (1 ‚àí ‚ÄñT‚Äñ) ‚àí Œ≤ ¬∑ (1 + ‚ÄñT‚Äñ¬≤) ¬∑ ‚ÄñT‚Äñ ¬∑ w_blame,i
œÅ_base,i ‚Üê œÅ_base,i + ŒîœÅ
```

Health and weight updates are decoupled. Health controls structural participation; W controls what is computed. A node can have learned excellent weights but low health (dormant but recoverable) or high health but poor weights (fires often but is still learning). These two axes of node state are independent.
Step 6: Atrophy
For all nodes:

```
steps_idle ‚Üê steps_idle + 1
if steps_idle > 50:
    œÅ_base,i ‚Üê œÅ_base,i ‚àí Œ≥ ¬∑ steps_idle
```

6. Phase 3 ‚Äî Structural Evolution
Structure evolves continuously. There is no separate architecture search phase ‚Äî MCT4 discovers its own depth, width, and topology during training.
6.1 Pruning
Any node with œÅ_base < 0 is deleted. All referencing edges are severed.
6.2 Capacity Insertion
When a node is pruned, new capacity is inserted where error pressure is highest, not where health is highest. This is the key structural principle: growth is driven by failure, not success.
Procedure:

1. Among all active edges (u ‚Üí v) in the last pass, find the one with the highest attributed tension magnitude from retrograde flow.

2. Spawn K new nodes (default K=2).

3. Each new node inherits W_new = W_u + ùí©(0, œÉ_mut ¬∑ I) and S_new = S_u + ùí©(0, œÉ_mut). Inheriting the upstream weight matrix gives new nodes a working starting point rather than random initialization, drastically shortening their learning warmup.

4. With 20% probability, assign a randomly different primitive to promote functional diversity.

5. Wire: u ‚Üí new ‚Üí v. Remove u ‚Üí v.

6. Incremental DFS acyclicity check. On cycle detection, skip this spawn (rare with DAG insertion).

7. Initialize œÅ_base,new = œÅ_base,u ¬∑ 0.5, tension_trace_new = tension_trace_u.

6.3 Lateral Wiring
Beyond depth insertion, MCT4 can grow laterally. Each pass, if a node i has tension_trace_i > œÑ_lateral (e.g., 0.3) for more than 20 consecutive passes, it spawns one additional outbound edge to a random downstream node that it is not already connected to (DAG-preserving). This creates shortcut paths for persistent error signals, equivalent to the skip connections that made ResNet trainable ‚Äî but discovered dynamically rather than designed in.
6.4 Convergence Dampening
When Œ∫ > Œ∫_thresh: œÉ_mut ‚Üê œÉ_mut ¬∑ 0.5, Œ≥ ‚Üê Œ≥ ¬∑ 0.5. The graph crystallizes. Dampening lifts on the next pruning event.
7. Primitive Operator Set
All operators output ‚Ñù·¥∞. Post-W application, the primitive is the nonlinearity. Choosing a primitive is choosing the activation function ‚Äî but nodes can mutate primitives, so the graph searches over activation functions as part of training.
Unary (1 input):
Op Definition Role ReLU(X) max(0, X) elementwise Sparse activation, fast Tanh(X) tanh(X) elementwise Bounded nonlinearity GELU(X) X ¬∑ Œ¶(X) elementwise Smooth gating, transformer-grade Softmax(X) standard softmax Probability / attention normalization L2Norm(X) X / ‚ÄñX‚Äñ‚ÇÇ Directional normalization Fork(X) Pass-through with temporary fan-out to ‚â§F downstream nodes Exploratory branching
Binary/N-ary (2+ inputs; aggregate >2 via mean before op):
Op Definition Role Add(X,Y) X + Y Residual connection Attention(X,Y) softmax(XY·µÄ/‚àöD) ¬∑ Y, pooled to D Full attention operation Gate(X,Y) X ‚äô œÉ(Y) Multiplicative gating Concat(X,Y) concat then mean-pool to D Feature fusion
The Attention primitive makes a single node capable of implementing a full attention head. A cluster of nodes with Attention primitives wired in parallel is equivalent to multi-head attention ‚Äî discovered structurally if the task requires it.
8. Handling Standard Deep Learning Tasks
8.1 Supervised Classification
Input node receives X. Output node emits logit vector Y ‚àà ‚Ñù·∂ú (or ‚Ñù·¥∞ projected to C classes by a fixed linear readout). Target Y* is the one-hot label. Standard cross-entropy tension: T_v = softmax(Y) ‚àí Y*. The graph learns depth and width appropriate to the task.
8.2 Sequence Modeling (Language, Audio, Time Series)
Process token x‚Çú as one forward pass. Context C carries state between tokens within a sequence ‚Äî it is the recurrent memory. The graph does not need a separate architecture for sequences; the context vector is the recurrence. For autoregressive generation, the output node's emission at step t becomes the input at step t+1.
8.3 Vision
Patch-embed images (e.g., 16√ó16 patches ‚Üí D-dimensional vectors) and process each patch as a sequence token. The graph develops spatial routing structure through lateral wiring and capacity insertion driven by spatial error patterns.
8.4 Batching and Scale
Run N samples simultaneously through the graph (Section 2.5). Weight updates aggregate over the batch. There is no theoretical limit on N other than memory. The graph itself can grow unboundedly via capacity insertion ‚Äî there is no D + M cap. Graph size is regulated entirely by atrophy: nodes that don't contribute get pruned. Scale is earned, not allocated.
9. Why the Local Learning Rule Is Sufficient
The central objection to any non-backprop learning rule is credit assignment: without the chain rule, how does an early node know its contribution to the final error? MCT4's answer has three components:
The retrograde signal carries directional information. T_v is a vector in ‚Ñù·¥∞, not a scalar. It encodes which directions in output space were wrong, not just how wrong. Each node that receives this signal updates its weights to reduce its contribution in those directions. This is richer than scalar error signals used in many proposed backprop alternatives.
The outer product update is the exact local gradient. For a node computing V_out = P(W V_in), the gradient of ‚ÄñT_local ‚àí V_out‚Äñ¬≤ with respect to W is T_local ‚äó V_in (ignoring the nonlinearity's Jacobian, which is element-wise bounded). The update rule is not a heuristic approximation ‚Äî it is stochastic gradient descent on the local objective. The approximation is in treating T_local as the target rather than the exact upstream gradient, which is the same approximation made by target propagation (Lee et al., 2015) ‚Äî a theoretically grounded alternative to backpropagation.
Structural evolution compensates for misalignment. If a node's weights cannot reduce its tension trace below threshold via learning alone, the graph grows a bypass route around it. Persistent high-tension nodes eventually become structurally isolated and pruned, replaced by new capacity with fresh initialization near the problem locus. The graph has a structural escape valve for learning stalls that gradient descent lacks entirely.
10. Hyperparameters
Parameter Symbol Default Description Vector dim D 512 Uniform dimensionality Context decay decay_c 0.95 Ghost signal half-life per step Catalysis Œ± 0.01 Health reward rate Solvent Œ≤ 0.05 Health penalty rate Atrophy Œ≥ 0.001 Idle decay per step Learning rate Œ∑ 0.001 Weight matrix update step size Weight max norm W_max ‚àöD Spectral norm ceiling for W Rank (factored) r 64 Low-rank factor dimension Mutation noise œÉ_mut 0.05 Spawn perturbation std dev Async decay Œª_async 0.2 Inbox signal decay rate Tau steepness Œª_œÑ 0.1 Latency kill-switch slope Spawn count K 2 New nodes per pruning event Fork fan-out F 2 Max temporary Fork edges Lateral tension threshold œÑ_lateral 0.3 Tension EMA to trigger wiring growth Convergence threshold Œ∫_thresh 100 Passes before dampening Batch size N 32 Samples per weight update
11. Complexity and Hardware Mapping
Per forward pass: O(k ¬∑ h ¬∑ N_active) where k = mean out-degree, h = mean path depth ‚â§ t_budget, N_active = nodes that fire.
Per weight update: O(D¬≤ ¬∑ N_active) for full-rank, O(Dr ¬∑ N_active) for low-rank factored.
GPU mapping: Nodes that fire in the same hop are independent and can be batched into a single matmul. Represent each hop as a sparse matrix-vector product over the active frontier. This maps to cusparse or torch.sparse operations. The priority queue determines hop assignment; nodes at the same hop depth execute in parallel. With h=20 hops and k=4, a graph of 512 nodes executes in 20 sparse matmul rounds ‚Äî comparable to a 20-layer transformer block in operation count, with substantially lower memory due to sparsity and absence of KV cache.
Memory: No activation storage for the backward pass. MCT4 requires only current inbox values and the context vector C during forward execution. Total inference memory is O(D ¬∑ N_active) plus O(D¬≤) per node for weights. A graph of 1000 nodes at D=512 requires ~1GB for full-rank weights ‚Äî manageable on a single GPU.
12. Implementation Bootstrap
Start with the minimal viable graph: two input nodes (one for content X, one for context C), four intermediate nodes (two GELU, one Gate, one Add), one output node. Wire them as a shallow DAG. Let structural evolution grow from there. Do not pre-design depth or width. The task will impose structure through the tension signal.
Core data structures:

* Node registry: Hash map id ‚Üí Node.

* Adjacency: CSR sparse list per node.

* Execution queue: Min-heap on hop count, with per-hop batch aggregation.

* Context: Single shared ‚Ñù·¥∞ vector with atomic updates under multi-threading.

* Batch buffer: Per-node N √ó D matrix of inbox slots.

Implementation languages in order of recommendation: Rust (memory safety + concurrency), C++ with BLAS, Python with torch.sparse (prototyping only ‚Äî Python's GIL makes true asynchrony require multiprocessing).

